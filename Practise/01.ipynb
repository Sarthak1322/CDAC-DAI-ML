{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b81171",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1. \tConsider handwritten digits dataset full_mnist_digits.csv file\t(5 Marks each)  \t\t(25 marks)\n",
    "\n",
    "    a. Design SVC, Random Forest Classifier, Logistic Regression classifier to classify digits. \n",
    "    b. Find accuracy_score for every model designed in option a.\n",
    "    c. Display classification report , which model is best \n",
    "    d. Display confusion matrix for all models.\n",
    "    e. For Random Forest Classifier use values (5,10,20,25,40,50) for n_estimators, display which value is best suitable for digits data. (use train_test_split)        \n",
    "Consider 70% data as training data set.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('full_mnist_digits.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = data.iloc[:, :-1]  # Assuming all columns except the last are features\n",
    "y = data.iloc[:, -1]   # Assuming the last column is the target\n",
    "\n",
    "# Split the dataset into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize classifiers\n",
    "svc = SVC()\n",
    "rf = RandomForestClassifier()\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train and evaluate SVC\n",
    "svc.fit(X_train, y_train)\n",
    "svc_pred = svc.predict(X_test)\n",
    "print(\"SVC Accuracy:\", accuracy_score(y_test, svc_pred))\n",
    "print(\"SVC Classification Report:\\n\", classification_report(y_test, svc_pred))\n",
    "print(\"SVC Confusion Matrix:\\n\", confusion_matrix(y_test, svc_pred))\n",
    "\n",
    "# Train and evaluate Logistic Regression\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, lr_pred))\n",
    "print(\"Logistic Regression Classification Report:\\n\", classification_report(y_test, lr_pred))\n",
    "print(\"Logistic Regression Confusion Matrix:\\n\", confusion_matrix(y_test, lr_pred))\n",
    "\n",
    "# Train and evaluate Random Forest Classifier with different n_estimators\n",
    "n_estimators = [5, 10, 20, 25, 40, 50]\n",
    "best_accuracy = 0\n",
    "best_n = 0\n",
    "\n",
    "for n in n_estimators:\n",
    "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, rf_pred)\n",
    "    print(f\"Random Forest Accuracy with n_estimators={n}: {acc}\")\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_n = n\n",
    "\n",
    "    print(f\"Random Forest Classification Report (n_estimators={n}):\\n\", classification_report(y_test, rf_pred))\n",
    "    print(f\"Random Forest Confusion Matrix (n_estimators={n}):\\n\", confusion_matrix(y_test, rf_pred))\n",
    "\n",
    "print(f\"Best n_estimators for Random Forest: {best_n} with accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q2. \tUse iris dataset and do the following\t\t\t                                                                        (15 marks)\n",
    "    f. Using elbow method find optimum number of clusters, to design k-Means clustering model.\n",
    "    g. Build model using k-means clustering, (use number of clusters found in question a)\n",
    "    h. Find silhouette_score to measure accuracy, of the k-means clustering model.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89319b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Step 1: Use the elbow method to find the optimal number of clusters\n",
    "inertia = []\n",
    "range_clusters = range(1, 11)\n",
    "\n",
    "for k in range_clusters:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range_clusters, inertia, marker='o')\n",
    "plt.title('Elbow Method to Determine Optimal Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Build the k-Means model using the optimal number of clusters (e.g., 3)\n",
    "optimal_clusters = 3  # Based on the elbow method\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Step 3: Evaluate the model using silhouette score\n",
    "labels = kmeans.labels_\n",
    "sil_score = silhouette_score(X, labels)\n",
    "print(f\"Silhouette Score for k-Means with {optimal_clusters} clusters: {sil_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a96d205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1.  Consider diabetes.csv data set. (5 Marks each)  \t\t                                                                        (25 marks)\n",
    "\n",
    "    a. Build a classification model using support vector machine. Use standalone model, find score.\n",
    "    b. Build Bagging model using SVM and check if you see any difference in the performance.\n",
    "    c. Use decision tree classifier. Use standalone model.\n",
    "    d. Build Bagging model using decision tree and check if you notice any difference in performance.\n",
    "    e. For Random Forest Classifier use values(100,120,200,300) for n_estimators, display which value is best suitable for given data . (use train_test_split) \n",
    "Consider 70% data as training data set.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d614d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = data.iloc[:, :-1]  # Assuming all columns except the last are features\n",
    "y = data.iloc[:, -1]   # Assuming the last column is the target\n",
    "\n",
    "# Split the dataset into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Task a: Standalone SVM\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "svm_pred = svm.predict(X_test)\n",
    "print(\"Standalone SVM Accuracy:\", accuracy_score(y_test, svm_pred))\n",
    "print(\"Standalone SVM Classification Report:\\n\", classification_report(y_test, svm_pred))\n",
    "\n",
    "# Task b: Bagging with SVM\n",
    "bagging_svm = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=42)\n",
    "bagging_svm.fit(X_train, y_train)\n",
    "bagging_svm_pred = bagging_svm.predict(X_test)\n",
    "print(\"Bagging SVM Accuracy:\", accuracy_score(y_test, bagging_svm_pred))\n",
    "print(\"Bagging SVM Classification Report:\\n\", classification_report(y_test, bagging_svm_pred))\n",
    "\n",
    "# Task c: Standalone Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_pred = dt.predict(X_test)\n",
    "print(\"Standalone Decision Tree Accuracy:\", accuracy_score(y_test, dt_pred))\n",
    "print(\"Standalone Decision Tree Classification Report:\\n\", classification_report(y_test, dt_pred))\n",
    "\n",
    "# Task d: Bagging with Decision Tree\n",
    "bagging_dt = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
    "bagging_dt.fit(X_train, y_train)\n",
    "bagging_dt_pred = bagging_dt.predict(X_test)\n",
    "print(\"Bagging Decision Tree Accuracy:\", accuracy_score(y_test, bagging_dt_pred))\n",
    "print(\"Bagging Decision Tree Classification Report:\\n\", classification_report(y_test, bagging_dt_pred))\n",
    "\n",
    "# Task e: Random Forest with different n_estimators\n",
    "n_estimators = [100, 120, 200, 300]\n",
    "best_accuracy = 0\n",
    "best_n = 0\n",
    "\n",
    "for n in n_estimators:\n",
    "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, rf_pred)\n",
    "    print(f\"Random Forest Accuracy with n_estimators={n}: {acc}\")\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_n = n\n",
    "\n",
    "    print(f\"Random Forest Classification Report (n_estimators={n}):\\n\", classification_report(y_test, rf_pred))\n",
    "\n",
    "print(f\"Best n_estimators for Random Forest: {best_n} with accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71438b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Q2. Consider handwritten digits dataset available in the scikit-learn library, and do the following\t(15 marks)\n",
    "\n",
    "    a.  Using PCA  to reduce the dimensions. (use 2 components)\n",
    "    b.  Display the percentage of the explained variance and the variance (eigenvalues of the covariance matrix)\n",
    "    c.  Apply Logistic Regression on the PCA component data and find accuracy score.\n",
    " \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca3a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "X = digits.data  # Features\n",
    "y = digits.target  # Target labels\n",
    "\n",
    "# Step 1: Apply PCA to reduce dimensions to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Step 2: Display the percentage of explained variance and eigenvalues\n",
    "explained_variance = pca.explained_variance_ratio_ * 100\n",
    "eigenvalues = pca.explained_variance_\n",
    "print(\"Percentage of Explained Variance by each component:\", explained_variance)\n",
    "print(\"Eigenvalues of the covariance matrix:\", eigenvalues)\n",
    "\n",
    "# Step 3: Split the PCA-transformed data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Apply Logistic Regression on the PCA-transformed data\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Step 5: Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of Logistic Regression on PCA-transformed data:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73bade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"   \n",
    "\n",
    "Q1.\n",
    "On Titanic Dataset:                                                                                                                                [10 Marks] \n",
    "a. Perform One-hot Encoding Technique on eligible column in dataset. \n",
    "b. Identify which column has null/missing data and replace null/missing data with mean.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the Titanic dataset\n",
    "data = pd.read_csv('titanic.csv')  # Replace with the correct path to your Titanic dataset\n",
    "\n",
    "# Task a: Perform One-Hot Encoding on eligible columns\n",
    "# Identify categorical columns\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "print(\"Categorical Columns:\", categorical_columns)\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(drop='first', sparse=False), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "data_encoded = column_transformer.fit_transform(data)\n",
    "data_encoded = pd.DataFrame(data_encoded, columns=column_transformer.get_feature_names_out())\n",
    "print(\"Data after One-Hot Encoding:\\n\", data_encoded.head())\n",
    "\n",
    "# Task b: Identify columns with missing/null values and replace them with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_imputed = pd.DataFrame(imputer.fit_transform(data_encoded), columns=data_encoded.columns)\n",
    "print(\"Data after replacing missing values:\\n\", data_imputed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd24a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"   \n",
    "Q2. \n",
    "On Titanic Dataset:                                                                                            \t\t              [20 Marks] \n",
    "a. Build a prediction model using Random Forest Algorithm to solve problem “what sorts of male people were more likely to survive”. \n",
    "b. Create a Confusion Matrix for the above solution. \n",
    "c. Tune the parameters for the existing solution and compare the results\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a734b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Titanic dataset\n",
    "data = pd.read_csv('titanic.csv')  # Replace with the correct path to your Titanic dataset\n",
    "\n",
    "# Filter data for male passengers\n",
    "data = data[data['Sex'] == 'male']\n",
    "\n",
    "# Perform preprocessing\n",
    "# Handle missing values\n",
    "data['Age'].fillna(data['Age'].mean(), inplace=True)\n",
    "data['Fare'].fillna(data['Fare'].mean(), inplace=True)\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "data = pd.get_dummies(data, columns=['Embarked'], drop_first=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(['Survived', 'Name', 'Ticket', 'Cabin', 'Sex'], axis=1)  # Drop irrelevant columns\n",
    "y = data['Survived']\n",
    "\n",
    "# Split the dataset into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Task a: Build a Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy of Random Forest Classifier:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Task b: Create a Confusion Matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Task c: Tune the parameters using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_tuned = best_rf.predict(X_test)\n",
    "print(\"Accuracy of Tuned Random Forest Classifier:\", accuracy_score(y_test, y_pred_tuned))\n",
    "print(\"Classification Report (Tuned):\\n\", classification_report(y_test, y_pred_tuned))\n",
    "print(\"Confusion Matrix (Tuned):\\n\", confusion_matrix(y_test, y_pred_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd9b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q3. \t\t\t\t\t\t\t\t\t\t\t\t\t [10 Marks]\n",
    "\n",
    " On Iris dataset, perform a K-means clustering algorithm. Show the suitable number of centroids    \n",
    "required.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Step 1: Use the elbow method to find the optimal number of clusters\n",
    "inertia = []\n",
    "range_clusters = range(1, 11)\n",
    "\n",
    "for k in range_clusters:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Step 2: Plot the elbow curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range_clusters, inertia, marker='o')\n",
    "plt.title('Elbow Method to Determine Optimal Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Perform K-means clustering with the optimal number of clusters (e.g., 3)\n",
    "optimal_clusters = 3  # Based on the elbow method\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Display the cluster centers\n",
    "print(\"Cluster Centers:\\n\", kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"   \n",
    "Q1.\n",
    " On Titanic Dataset:                                                                                                                            [10 Marks]  \n",
    "a. Perform Binary Encoding Technique on eligible column in dataset. \n",
    "b. Identify which column has null/missing data and replace null/missing data with median.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e8f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the Titanic dataset\n",
    "data = pd.read_csv('titanic.csv')  # Replace with the correct path to your Titanic dataset\n",
    "\n",
    "# Task a: Perform Binary Encoding on eligible columns\n",
    "# Identify categorical columns\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "print(\"Categorical Columns:\", categorical_columns)\n",
    "\n",
    "# Apply Binary Encoding using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    data[col] = label_encoder.fit_transform(data[col].astype(str))\n",
    "\n",
    "print(\"Data after Binary Encoding:\\n\", data.head())\n",
    "\n",
    "# Task b: Identify columns with missing/null values and replace them with the median\n",
    "missing_columns = data.columns[data.isnull().any()]\n",
    "print(\"Columns with Missing Values:\", missing_columns)\n",
    "\n",
    "# Replace missing values with the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data[missing_columns] = imputer.fit_transform(data[missing_columns])\n",
    "\n",
    "print(\"Data after replacing missing values:\\n\", data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"   \n",
    "Q2\n",
    " On Titanic Dataset:                                                                                                                             [20 Marks] \n",
    "a. Build a prediction model using Random Forest Algorithm to solve problem “what sorts of female people were more likely to survive”. \n",
    "b. Create a Confusion Matrix for the above solution. \n",
    "c. Tune the parameters for the existing solution and compare the results.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb78c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Titanic dataset\n",
    "data = pd.read_csv('titanic.csv')  # Replace with the correct path to your Titanic dataset\n",
    "\n",
    "# Filter data for female passengers\n",
    "data = data[data['Sex'] == 'female']\n",
    "\n",
    "# Perform preprocessing\n",
    "# Handle missing values\n",
    "data['Age'].fillna(data['Age'].mean(), inplace=True)\n",
    "data['Fare'].fillna(data['Fare'].mean(), inplace=True)\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "data = pd.get_dummies(data, columns=['Embarked'], drop_first=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(['Survived', 'Name', 'Ticket', 'Cabin', 'Sex'], axis=1)  # Drop irrelevant columns\n",
    "y = data['Survived']\n",
    "\n",
    "# Split the dataset into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Task a: Build a Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy of Random Forest Classifier:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Task b: Create a Confusion Matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Task c: Tune the parameters using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_tuned = best_rf.predict(X_test)\n",
    "print(\"Accuracy of Tuned Random Forest Classifier:\", accuracy_score(y_test, y_pred_tuned))\n",
    "print(\"Classification Report (Tuned):\\n\", classification_report(y_test, y_pred_tuned))\n",
    "print(\"Confusion Matrix (Tuned):\\n\", confusion_matrix(y_test, y_pred_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceb59e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Q3.                                                                                                                                                       [10 Marks]\n",
    " On Iris dataset, perform a K-means clustering algorithm. Show the suitable number of centroids  \n",
    " required.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Step 1: Use the elbow method to find the optimal number of clusters\n",
    "inertia = []\n",
    "range_clusters = range(1, 11)\n",
    "\n",
    "for k in range_clusters:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Step 2: Plot the elbow curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range_clusters, inertia, marker='o')\n",
    "plt.title('Elbow Method to Determine Optimal Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Perform K-means clustering with the optimal number of clusters (e.g., 3)\n",
    "optimal_clusters = 3  # Based on the elbow method\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Display the cluster centers\n",
    "print(\"Cluster Centers:\\n\", kmeans.cluster_centers_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
